{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fast neural style  基于VGG16的快速图像风格化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.onnx\n",
    "\n",
    "import utils\n",
    "from transformer_net import TransformerNet\n",
    "from vgg import Vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入matplotlib库，实现可视化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_images_side_by_side(original_image_path, stylized_image_path, title1='Original Image', title2='Stylized Image'):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # 创建一个包含两个子图的图形\n",
    "    \n",
    "    # 加载并显示原始图像\n",
    "    original_img = mpimg.imread(original_image_path)\n",
    "    axs[0].imshow(original_img)\n",
    "    axs[0].set_title(title1)\n",
    "    axs[0].axis('off')  # 不显示坐标轴\n",
    "\n",
    "    # 加载并显示风格化后的图像\n",
    "    stylized_img = mpimg.imread(stylized_image_path)\n",
    "    axs[1].imshow(stylized_img)\n",
    "    axs[1].set_title(title2)\n",
    "    axs[1].axis('off')  # 不显示坐标轴\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改训练函数\n",
    "首先，导入所需库，并准备一个简单的绘图函数来实时更新训练损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def plot_losses(epochs, content_losses, style_losses, total_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, content_losses, label='Content Loss')\n",
    "    plt.plot(epochs, style_losses, label='Style Loss')\n",
    "    plt.plot(epochs, total_losses, label='Total Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查路径函数\n",
    "此函数检查和创建存储模型的目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_paths(args):\n",
    "    try:\n",
    "        if not os.path.exists(args.save_model_dir):  # 如果存储模型的目录不存在\n",
    "            os.makedirs(args.save_model_dir)  # 创建目录\n",
    "        if args.checkpoint_model_dir is not None and not (os.path.exists(args.checkpoint_model_dir)):\n",
    "            os.makedirs(args.checkpoint_model_dir)  # 创建检查点目录\n",
    "    except OSError as e:\n",
    "        print(e)    \n",
    "        sys.exit(1)  # 出错则退出程序\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数\n",
    "此函数封装了整个训练流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(args.image_size),\n",
    "        transforms.CenterCrop(args.image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    train_dataset = datasets.ImageFolder(args.dataset, transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    transformer = TransformerNet().to(device)\n",
    "    optimizer = Adam(transformer.parameters(), args.lr)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    vgg = Vgg16(requires_grad=False).to(device)\n",
    "\n",
    "    style_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    style = utils.load_image(args.style_image, size=args.style_size)\n",
    "    style = style_transform(style).repeat(args.batch_size, 1, 1, 1).to(device)\n",
    "    features_style = vgg(utils.normalize_batch(style))\n",
    "    gram_style = [utils.gram_matrix(y) for y in features_style]\n",
    "\n",
    "    content_losses, style_losses, total_losses, epochs = [], [], [], []\n",
    "\n",
    "    for e in range(args.epochs):\n",
    "        transformer.train()\n",
    "        epoch_content_loss, epoch_style_loss, count = 0.0, 0.0, 0\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch {e + 1}/{args.epochs}', unit='batch') as pbar:\n",
    "            for batch_id, (x, _) in enumerate(train_loader):\n",
    "                n_batch = len(x)\n",
    "                count += n_batch\n",
    "                optimizer.zero_grad()\n",
    "                x = x.to(device)\n",
    "                y = transformer(x)\n",
    "\n",
    "                y = utils.normalize_batch(y)\n",
    "                x = utils.normalize_batch(x)\n",
    "\n",
    "                features_y = vgg(y)\n",
    "                features_x = vgg(x)\n",
    "\n",
    "                content_loss = args.content_weight * mse_loss(features_y.relu2_2, features_x.relu2_2)\n",
    "                style_loss = 0.\n",
    "                for ft_y, gm_s in zip(features_y, gram_style):\n",
    "                    gm_y = utils.gram_matrix(ft_y)\n",
    "                    style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])\n",
    "                style_loss *= args.style_weight\n",
    "\n",
    "                total_loss = content_loss + style_loss\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_content_loss += content_loss.item() * n_batch\n",
    "                epoch_style_loss += style_loss.item() * n_batch\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        # 记录每个epoch的平均损失\n",
    "        epoch_content_loss /= len(train_dataset)\n",
    "        epoch_style_loss /= len(train_dataset)\n",
    "        epoch_total_loss = epoch_content_loss + epoch_style_loss\n",
    "        content_losses.append(epoch_content_loss)\n",
    "        style_losses.append(epoch_style_loss)\n",
    "        total_losses.append(epoch_total_loss)\n",
    "        epochs.append(e + 1)\n",
    "\n",
    "        # 更新绘图\n",
    "        plot_losses(epochs, content_losses, style_losses, total_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 风格化函数\n",
    "用于将已训练的模型应用于新的内容图像，实现风格转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize(args):\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    content_image = utils.load_image(args.content_image, scale=args.content_scale)\n",
    "    content_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    content_image = content_transform(content_image)\n",
    "    content_image = content_image.unsqueeze(0).to(device)\n",
    "\n",
    "    if args.model.endswith(\".onnx\"):\n",
    "        output = stylize_onnx(content_image, args)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            style_model = TransformerNet()\n",
    "            state_dict = torch.load(args.model)\n",
    "            for k in list(state_dict.keys()):\n",
    "                if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "                    del state_dict[k]\n",
    "            style_model.load_state_dict(state_dict)\n",
    "            style_model.to(device)\n",
    "            style_model.eval()\n",
    "            if args.export_onnx:\n",
    "                assert args.export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n",
    "                output = torch.onnx._export(style_model, content_image, args.export_onnx, opset_version=11).cpu()            \n",
    "            else:\n",
    "                output = style_model(content_image).cpu()\n",
    "\n",
    "    # 保存并显示图像\n",
    "    utils.save_image(args.output_image, output[0])\n",
    "    show_images_side_by_side(args.content_image, args.output_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX风格化函数\n",
    "使用ONNX运行时进行风格化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize_onnx(content_image, args):\n",
    "    \"\"\"\n",
    "    Read ONNX model and run it using onnxruntime\n",
    "    \"\"\"\n",
    "\n",
    "    assert not args.export_onnx\n",
    "\n",
    "    import onnxruntime\n",
    "\n",
    "    ort_session = onnxruntime.InferenceSession(args.model)\n",
    "\n",
    "    def to_numpy(tensor):\n",
    "        return (\n",
    "            tensor.detach().cpu().numpy()\n",
    "            if tensor.requires_grad\n",
    "            else tensor.cpu().numpy()\n",
    "        )\n",
    "\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(content_image)}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    img_out_y = ort_outs[0]\n",
    "\n",
    "    return torch.from_numpy(img_out_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主函数\n",
    "解析命令行参数，根据用户选择执行训练或风格化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    main_arg_parser = argparse.ArgumentParser(description=\"parser for fast-neural-style\")\n",
    "    subparsers = main_arg_parser.add_subparsers(title=\"subcommands\", dest=\"subcommand\")\n",
    "\n",
    "    train_arg_parser = subparsers.add_parser(\"train\", help=\"parser for training arguments\")\n",
    "    train_arg_parser.add_argument(\"--epochs\", type=int, default=2,\n",
    "                                  help=\"number of training epochs, default is 2\")\n",
    "    train_arg_parser.add_argument(\"--batch-size\", type=int, default=4,\n",
    "                                  help=\"batch size for training, default is 4\")\n",
    "    train_arg_parser.add_argument(\"--dataset\", type=str, required=True,\n",
    "                                  help=\"path to training dataset, the path should point to a folder \"\n",
    "                                       \"containing another folder with all the training images\")\n",
    "    train_arg_parser.add_argument(\"--style-image\", type=str, default=\"images/style-images/mosaic.jpg\",\n",
    "                                  help=\"path to style-image\")\n",
    "    train_arg_parser.add_argument(\"--save-model-dir\", type=str, required=True,\n",
    "                                  help=\"path to folder where trained model will be saved.\")\n",
    "    train_arg_parser.add_argument(\"--checkpoint-model-dir\", type=str, default=None,\n",
    "                                  help=\"path to folder where checkpoints of trained models will be saved\")\n",
    "    train_arg_parser.add_argument(\"--image-size\", type=int, default=256,\n",
    "                                  help=\"size of training images, default is 256 X 256\")\n",
    "    train_arg_parser.add_argument(\"--style-size\", type=int, default=None,\n",
    "                                  help=\"size of style-image, default is the original size of style image\")\n",
    "    train_arg_parser.add_argument(\"--cuda\", type=int, required=True,\n",
    "                                  help=\"set it to 1 for running on GPU, 0 for CPU\")\n",
    "    train_arg_parser.add_argument(\"--seed\", type=int, default=42,\n",
    "                                  help=\"random seed for training\")\n",
    "    train_arg_parser.add_argument(\"--content-weight\", type=float, default=1e5,\n",
    "                                  help=\"weight for content-loss, default is 1e5\")\n",
    "    train_arg_parser.add_argument(\"--style-weight\", type=float, default=1e10,\n",
    "                                  help=\"weight for style-loss, default is 1e10\")\n",
    "    train_arg_parser.add_argument(\"--lr\", type=float, default=1e-3,\n",
    "                                  help=\"learning rate, default is 1e-3\")\n",
    "    train_arg_parser.add_argument(\"--log-interval\", type=int, default=500,\n",
    "                                  help=\"number of images after which the training loss is logged, default is 500\")\n",
    "    train_arg_parser.add_argument(\"--checkpoint-interval\", type=int, default=2000,\n",
    "                                  help=\"number of batches after which a checkpoint of the trained model will be created\")\n",
    "\n",
    "    eval_arg_parser = subparsers.add_parser(\"eval\", help=\"parser for evaluation/stylizing arguments\")\n",
    "    eval_arg_parser.add_argument(\"--content-image\", type=str, required=True,\n",
    "                                 help=\"path to content image you want to stylize\")\n",
    "    eval_arg_parser.add_argument(\"--content-scale\", type=float, default=None,\n",
    "                                 help=\"factor for scaling down the content image\")\n",
    "    eval_arg_parser.add_argument(\"--output-image\", type=str, required=True,\n",
    "                                 help=\"path for saving the output image\")\n",
    "    eval_arg_parser.add_argument(\"--model\", type=str, required=True,\n",
    "                                 help=\"saved model to be used for stylizing the image. If file ends in .pth - PyTorch path is used, if in .onnx - Caffe2 path\")\n",
    "    eval_arg_parser.add_argument(\"--cuda\", type=int, default=False,\n",
    "                                 help=\"set it to 1 for running on cuda, 0 for CPU\")\n",
    "    eval_arg_parser.add_argument(\"--export_onnx\", type=str,\n",
    "                                 help=\"export ONNX model to a given file\")\n",
    "    eval_arg_parser.add_argument('--mps', action='store_true', default=False, help='enable macOS GPU training')\n",
    "\n",
    "    args = main_arg_parser.parse_args()\n",
    "\n",
    "    if args.subcommand is None:\n",
    "        print(\"ERROR: specify either train or eval\")\n",
    "        sys.exit(1)\n",
    "    if args.cuda and not torch.cuda.is_available():\n",
    "        print(\"ERROR: cuda is not available, try running on CPU\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if args.subcommand == \"train\":\n",
    "        check_paths(args)\n",
    "        train(args)\n",
    "    else:\n",
    "        stylize(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像风格化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "def stylize_image(content_image, model, output_image, cuda, content_scale=None, export_onnx=None):\n",
    "    args = Namespace(\n",
    "        content_image=content_image,\n",
    "        model=model,\n",
    "        output_image=output_image,\n",
    "        cuda=cuda,\n",
    "        content_scale=content_scale,\n",
    "        export_onnx=export_onnx\n",
    "    )\n",
    "\n",
    "    # 调用原先的 stylize 函数\n",
    "    stylize(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像风格化训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "def train_model(dataset, style_image, save_model_dir, epochs, cuda):\n",
    "    args = Namespace(\n",
    "        dataset=dataset,\n",
    "        style_image=style_image,\n",
    "        save_model_dir=save_model_dir,\n",
    "        epochs=epochs,\n",
    "        cuda=cuda,\n",
    "        batch_size=4,  # 可以根据需要设置默认值\n",
    "        lr=1e-3,       # 学习率默认值\n",
    "        content_weight=1e5,\n",
    "        style_weight=1e10,\n",
    "        image_size=256,\n",
    "        style_size=None,\n",
    "        seed=42,\n",
    "        checkpoint_model_dir=None,  # 如果需要可以提供\n",
    "        log_interval=500,\n",
    "        checkpoint_interval=2000\n",
    "    )\n",
    "    \n",
    "    # 检查并创建必要的目录\n",
    "    check_paths(args)\n",
    "    \n",
    "    # 执行训练\n",
    "    train(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2664487102.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    stylize_image(\n",
    "        content_image=\"../images/content_images/amber.jpg\",\n",
    "        model=\"../new_saved_models/Monet.model\",\n",
    "        output_image=\"../images/output_images/amber-monet.jpg\",\n",
    "        cuda=1\n",
    "    )\n",
    "    '''\n",
    "    train_model(\n",
    "        dataset='../../train', # 设置数据集位置\n",
    "        style_image='../images/style_images/cartoon.png', # 设置风格化位置\n",
    "        save_model_dir='../new_saved_models', # 设置文件保存位置\n",
    "        epochs=2,\n",
    "        cuda=1\n",
    "    )\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_enhance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
